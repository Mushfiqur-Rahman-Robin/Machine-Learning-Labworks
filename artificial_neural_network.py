# -*- coding: utf-8 -*-
"""Artificial Neural Network

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c_SJcdwh8lQ7k0ILccMNHA6fINLlI74F
"""

import matplotlib.pyplot as plt
import pandas as pd
import pylab as pl
import numpy as np

import warnings
warnings.filterwarnings('ignore')

my_data = pd.read_csv("/content/depression_ml.csv",header=0, delimiter=",")
my_data.head()

my_data = my_data.drop(['Timestamp', 'Gender'], axis = 1)
my_data

my_data.describe()

my_data.columns

from sklearn import preprocessing
oh = preprocessing.OneHotEncoder()
le = preprocessing.LabelEncoder()

selected_col = list(my_data.columns)
to_delete = [2,3,7,18]

selected_col = np.delete(selected_col, to_delete).tolist()
selected_col

my_data[selected_col] = my_data[selected_col].apply(le.fit_transform)
my_data

import seaborn as sns

corr = my_data.corr()
plt.figure(figsize=(25,20))
sns.heatmap(corr,annot=True,cmap='Reds')

X = my_data.iloc[:, np.r_[0:2, 3:19]]
X

Y = my_data.iloc[:,2]
Y

from sklearn.model_selection import train_test_split

X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, Y, test_size=0.3, random_state=3)

X_trainset = X_trainset.to_numpy()
y_trainset = y_trainset.to_numpy()
X_testset = X_testset.to_numpy()
y_testset = y_testset.to_numpy()

from keras.models import Sequential
from keras.layers import Dense, Dropout
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from tensorflow.keras.optimizers import Adam

import tensorflow as tf
class CustomCallbacks(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
      if(logs.get('accuracy') > 0.99):
        print("\n 99% acc reached")
        self.model.stop_training = True
        
def create_model(learning_rate, dropout_rate):
  model = Sequential()
  model.add(Dense(19, input_shape=(X_trainset.shape[0], X_testset.shape[1]), activation='relu'))
  model.add(Dense(32, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(dropout_rate))
  model.add(Dense(100, activation='softmax'))

  model.compile(optimizer= Adam(learning_rate = 0.001),
              loss='sparse_categorical_crossentropy',
              metrics = ['accuracy'])
  return model

from keras.wrappers.scikit_learn import KerasClassifier
model = KerasClassifier(build_fn=create_model, verbose=1)

from sklearn.model_selection import GridSearchCV

learning_rate = [0.001, 0.01]
dropout_rate = [0.0, 0.2, 0.4]
batch_size = [10, 20]
epochs = [30, 50]

seed = 42

# Make a dictionary of the grid search parameters
param_grid = dict(learning_rate=learning_rate, dropout_rate=dropout_rate, batch_size=batch_size, epochs=epochs)

# Build and fit the GridSearchCV
grid = GridSearchCV(model, param_grid=param_grid,
                    cv = 3, verbose = 1, scoring="accuracy")

grid_results = grid.fit(X, Y)

# Summarize the results in a readable format
print("Best: {0}, using {1}".format(grid_results.best_score_, grid_results.best_params_))

model = Sequential()
model.add(Dense(19, input_shape=(X_trainset.shape[0], X_testset.shape[1]), activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(100, activation='softmax'))

model.compile(optimizer= Adam(learning_rate = 0.001),
            loss='sparse_categorical_crossentropy',
            metrics = ['accuracy'])

model.summary()

history = model.fit(
    X_trainset,
    y_trainset,
    epochs = 100,
    batch_size = 20,
    validation_data=(X_testset, y_testset),
    verbose=1
)

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

model.evaluate(X_trainset, y_trainset)

model.evaluate(X_testset, y_testset)